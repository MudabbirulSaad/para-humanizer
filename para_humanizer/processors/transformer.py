"""
Transformer-based processor for UltimateParaphraser.
Provides functionality for AI model-based text paraphrasing.
"""
import torch
from transformers import (
    PegasusForConditionalGeneration,
    PegasusTokenizer,
    T5ForConditionalGeneration,
    T5Tokenizer,
    AutoTokenizer, 
    AutoModelForSeq2SeqLM
)
import random
import logging
import re
import time
from typing import List, Dict, Tuple, Any, Optional, Union

from para_humanizer.utils.config import DEFAULT_TRANSFORMER_MODEL
from para_humanizer.utils.text_utils import fix_formatting

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TransformerProcessor:
    """
    Implements transformer-based methods for text paraphrasing
    using Hugging Face transformer models.
    """
    
    def __init__(self, device: str = "cpu", model_name: str = DEFAULT_TRANSFORMER_MODEL):
        """
        Initialize the transformer processor.
        
        Args:
            device: The device to use for model inference ('cuda' or 'cpu')
            model_name: Name of the pre-trained model to use
        """
        self.device = device
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.initialized = False
        
        # Attempt to initialize the model
        self._init_model()
        
    def _init_model(self) -> None:
        """
        Initialize the transformer model with better error handling.
        """
        try:
            # Log initialization attempt
            logger.info(f"Initializing transformer model {self.model_name} on {self.device}")
            
            if "pegasus" in self.model_name.lower():
                self.tokenizer = PegasusTokenizer.from_pretrained(self.model_name)
                self.model = PegasusForConditionalGeneration.from_pretrained(self.model_name).to(self.device)
            elif "t5" in self.model_name.lower():
                self.tokenizer = T5Tokenizer.from_pretrained(self.model_name)
                self.model = T5ForConditionalGeneration.from_pretrained(self.model_name).to(self.device)
            else:
                # Generic Auto classes for other models
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name).to(self.device)
                
            # Set model to evaluation mode
            self.model.eval()
            
            # Set initialization flag
            self.initialized = True
            logger.info("Transformer model initialized successfully")
            
        except Exception as e:
            logger.error(f"Error initializing transformer model: {str(e)}")
            self.initialized = False
    
    def _clean_transformer_output(self, text: str) -> str:
        """
        Clean up the transformer model output.
        
        Args:
            text: The text generated by the transformer
            
        Returns:
            Cleaned text
        """
        # Remove extra spaces
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Sometimes transformers add artifacts like [CLS], [SEP], etc.
        text = re.sub(r'\[\w+\]', '', text).strip()
        
        # Fix common transformer issues (capitalization, trailing punctuation)
        if text and text[0].islower():
            text = text[0].upper() + text[1:]
            
        # Make sure text ends with proper punctuation
        if text and not text.endswith(('.', '!', '?')):
            text += '.'
            
        return text
    
    def is_initialized(self) -> bool:
        """
        Check if the transformer model is initialized.
        
        Returns:
            True if the model is initialized and ready for use
        """
        return self.initialized and self.model is not None and self.tokenizer is not None
    
    def paraphrase(self, text: str, num_return_sequences: int = 3, 
                  max_length: int = 60) -> Optional[str]:
        """
        Paraphrase text using the transformer model with better error handling.
        
        Args:
            text: The text to paraphrase
            num_return_sequences: Number of paraphrase candidates to generate
            max_length: Maximum length of input text to process in one pass
            
        Returns:
            Paraphrased text or None if an error occurs
        """
        # Check if model is initialized
        if not self.is_initialized():
            logger.warning("Transformer model not initialized, cannot paraphrase")
            return None
            
        # Check for empty or invalid text
        if not text or not text.strip():
            return ""
            
        try:
            # Limit input length to avoid token limit issues
            if len(text) > max_length * 4:  # Rough character to token ratio
                text = text[:max_length * 4]
                
            # Tokenize the input text
            inputs = self.tokenizer(
                text, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=max_length
            ).to(self.device)
            
            # Generate paraphrases with diverse settings
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs["input_ids"],
                    num_return_sequences=num_return_sequences,
                    num_beams=num_return_sequences * 2,
                    max_length=max_length * 2,
                    temperature=0.8,  # Slightly higher temperature for more variety
                    top_k=50,
                    top_p=0.95,
                    repetition_penalty=1.2,  # Discourage repetition
                    no_repeat_ngram_size=2,
                    early_stopping=True
                )
                
            # Decode the outputs and select the best one
            paraphrases = [
                self.tokenizer.decode(output, skip_special_tokens=True)
                for output in outputs
            ]
            
            # Clean up the paraphrases
            cleaned_paraphrases = [self._clean_transformer_output(p) for p in paraphrases]
            
            # Filter out exact duplicates or empty results
            unique_paraphrases = [p for p in cleaned_paraphrases if p and p.strip()]
            
            if not unique_paraphrases:
                logger.warning("Transformer generated no valid paraphrases")
                return text  # Return original if no valid paraphrases
                
            # Return a random paraphrase for variety
            result = random.choice(unique_paraphrases)
            
            # Final formatting fixes
            result = fix_formatting(result)
            
            return result
            
        except Exception as e:
            logger.error(f"Error in transformer paraphrasing: {str(e)}")
            return None
    
    def paraphrase_chunks(self, chunks: List[str], max_length: int = 60) -> List[str]:
        """
        Paraphrase a list of text chunks.
        
        Args:
            chunks: List of text chunks to paraphrase
            max_length: Maximum length of input text to process in one pass
            
        Returns:
            List of paraphrased chunks
        """
        results = []
        
        for chunk in chunks:
            # Add slight delay between chunks to prevent overloading
            time.sleep(0.1)
            
            result = self.paraphrase(chunk, max_length=max_length)
            
            # Use original if paraphrasing fails
            if result is None:
                results.append(chunk)
            else:
                results.append(result)
                
        return results
